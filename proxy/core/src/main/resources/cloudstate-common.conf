// Common configuration to be included by other impls

akka {
  actor {
    provider = cluster
    allow-java-serialization = off
    warn-about-java-serializer-usage = off
    internal-dispatcher = akka.actor.default-dispatcher
    serializers {
      crdt-serializers = "io.cloudstate.proxy.crdt.CrdtSerializers"
      proto-any = "io.cloudstate.proxy.ProtobufAnySerializer"
    }
    serialization-bindings {
      "com.google.protobuf.any.Any" = proto-any
      "scalapb.GeneratedMessage" = proto
      "io.cloudstate.proxy.crdt.Vote" = crdt-serializers
    }
    serialization-identifiers {
      "io.cloudstate.proxy.crdt.CrdtSerializers" = 266587645 // hashCode of the classname
      "io.cloudstate.proxy.ProtobufAnySerializer" = 1222419123 // hashCode of the classname
    }
  }

  coordinated-shutdown.exit-jvm = on

  http.server {
    preview.enable-http2 = on

    // It would be better to disable this altogether, but without https://github.com/akka/akka-http/issues/2640,
    // that's a connection leak waiting to happen. Also, due to https://github.com/akka/akka-grpc/issues/664,
    // idle-timeout causes bugs in our streamed connection support.
    idle-timeout = 12h

    // This is an attempt to mitigate the lack of https://github.com/akka/akka-http/issues/2640 with the high idle
    // timeout above, not sure if it will work through the platform load balancers.
    socket-options.tcp-keep-alive = true
  }

  remote {
    // Avoid Netty being loaded
    enabled-transports = []
    artery {
      // FIXME do we need to set canonical.hostname?
      // FIXME do we need to set advanced.tcp.connection-timeout?
      canonical.port = ${?REMOTING_PORT}
      bind.port = ${?REMOTING_PORT}
    }
  }

  cluster {
    shutdown-after-unsuccessful-join-seed-nodes = 60s

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    split-brain-resolver.active-strategy = keep-majority

    sharding.state-store-mode = ddata

    // Non-durable for now, since we can't get native-image to work with lmdb right now
    sharding.distributed-data.durable.keys = []

    sharding {
      rebalance-interval = 5s
      passivate-idle-entity-after = off // FIXME put in a good value here
      # the dispatcher alias for internal-dispatcher doesn't work with shards (since Akka 2.6.8)
      # work around this by setting the internal sharding dispatcher to default-dispatcher directly
      # FIXME: setting can be removed again once fixed in Akka
      use-dispatcher = akka.actor.default-dispatcher
    }

    // Native image doesn't support JMX
    jmx.enabled = off
  }

  management {
    http.port = ${?MANAGEMENT_PORT}
    cluster.bootstrap {
      contact-point-discovery {
        discovery-method = kubernetes-api
        service-name = ${?SELECTOR_LABEL_VALUE}
        required-contact-point-nr = ${?REQUIRED_CONTACT_POINT_NR}
        # This is quite short, but in Knative we want to scale from zero very quickly.
        # todo work out whether Knative will increment scale from zero by more than one,
        # if it will, this could be problematic.
        stable-margin = 1s
      }
    }

    health-checks {
      readiness-checks.cloudstate = "io.cloudstate.proxy.HealthCheckReady"
      liveness-checks.cloudstate = "io.cloudstate.proxy.HealthCheckLive"
    }
  }

  discovery.kubernetes-api {
    pod-label-selector = ${?SELECTOR_LABEL}"=%s"
  }

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}
